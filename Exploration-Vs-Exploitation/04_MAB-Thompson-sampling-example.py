#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Aug 24 20:13:27 2019

@author: nitin
"""

import gym
import gym_bandits
import numpy as np

# number of rounds (iterations)
num_rounds = 20000

# Count of number of times an arm was pulled
count = np.zeros(10)

# Sum of rewards of each arm
sum_rewards = np.zeros(10)

# Q value which is the average reward
Q = np.zeros(10)

# Initialize alpha and beta values
alpha = np.ones(10)
beta = np.ones(10)


def thompson_sampling(alpha,beta):
    
    samples = [np.random.beta(alpha[i]+1,beta[i]+1) for i in range(10)]

    return np.argmax(samples)


env = gym.make("BanditTenArmedGaussian-v0")

env.reset()

# Start pulling the arm
for i in range(num_rounds):
    
    # Select the arm using thompson sampling
    arm = thompson_sampling(alpha,beta)
    
    # Get the reward
    observation, reward, done, info = env.step(arm) 
    
    # update the count of that arm
    count[arm] += 1
    
    # Sum the rewards obtained from the arm
    sum_rewards[arm]+=reward
    
    # calculate Q value which is the average rewards of the arm
    Q[arm] = sum_rewards[arm]/count[arm]

    # If it is a positive reward increment alpha
    if reward >0:
        alpha[arm] += 1
        
    # If it is a negative reward increment beta
    else:
        beta[arm] += 1
    
print( 'The optimal arm is {}'.format(np.argmax(Q)))



